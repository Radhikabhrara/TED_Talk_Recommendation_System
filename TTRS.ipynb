{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "53RsrQFxkwOc"
      },
      "source": [
        "Fetching Real time Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XQrFaagVk0Hy"
      },
      "outputs": [],
      "source": [
        "from googleapiclient.discovery import build\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "api_key= \"AIzaSyCjPkeYqEA4FqalBl9blpFs9Fnucp3kBUY\"\n",
        "#channel_id = \"UCsT0YIqwnpJCM-mx7-gSA4Q\"\n",
        "\n",
        "channel_ids = [\"UCsT0YIqwnpJCM-mx7-gSA4Q\",\n",
        "               \"UCAuUUnT6oDeKwE6v1NGQxug\",\n",
        "               \"UCsooa4yRKGN_zEE8iknghZA\",\n",
        "               \"UC-yTB2bUcin9mmah36sXiYA\"]        \n",
        "\n",
        "youtube = build('youtube', 'v3', developerKey=api_key)\n",
        "\n",
        "def get_channel_stats(youtube, channel_ids):\n",
        "    all_data = []\n",
        "    request = youtube.channels().list(\n",
        "                part='snippet,contentDetails,statistics',\n",
        "                id=','.join(channel_ids))\n",
        "    response = request.execute() \n",
        "    \n",
        "    for i in range(len(response['items'])):\n",
        "        data = dict(Channel_name = response['items'][i]['snippet']['title'],\n",
        "                    Subscribers = response['items'][i]['statistics']['subscriberCount'],\n",
        "                    Views = response['items'][i]['statistics']['viewCount'],\n",
        "                    Total_videos = response['items'][i]['statistics']['videoCount'],\n",
        "                    playlist_id = response['items'][i]['contentDetails']['relatedPlaylists']['uploads'])\n",
        "        all_data.append(data)\n",
        "    \n",
        "    return all_data\n",
        "\n",
        "\n",
        "channel_statistics = get_channel_stats(youtube, channel_ids)\n",
        "channel_data = pd.DataFrame(channel_statistics)\n",
        "\n",
        "playlist_id = channel_data.loc[channel_data['Channel_name']=='TEDx Talks', 'playlist_id'].iloc[0]\n",
        "\n",
        "def get_video_ids(youtube, playlist_id):\n",
        "    \n",
        "    request = youtube.playlistItems().list(\n",
        "                part='contentDetails',\n",
        "                playlistId = playlist_id,\n",
        "                maxResults = 50)\n",
        "    response = request.execute()\n",
        "    \n",
        "    video_ids = []\n",
        "    \n",
        "    for i in range(len(response['items'])):\n",
        "        video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
        "        \n",
        "    next_page_token = response.get('nextPageToken')\n",
        "    more_pages = True\n",
        "    \n",
        "    while more_pages:\n",
        "        if next_page_token is None:\n",
        "            more_pages = False\n",
        "        else:\n",
        "            request = youtube.playlistItems().list(\n",
        "                        part='contentDetails',\n",
        "                        playlistId = playlist_id,\n",
        "                        maxResults = 50,\n",
        "                        pageToken = next_page_token)\n",
        "            response = request.execute()\n",
        "    \n",
        "            for i in range(len(response['items'])):\n",
        "                video_ids.append(response['items'][i]['contentDetails']['videoId'])\n",
        "            \n",
        "            next_page_token = response.get('nextPageToken')\n",
        "        \n",
        "    return video_ids\n",
        "\n",
        "video_ids = get_video_ids(youtube, playlist_id)\n",
        "\n",
        "def get_video_details(youtube, video_ids):\n",
        "    all_video_stats = []\n",
        "    \n",
        "    for i in range(0, len(video_ids), 50):\n",
        "        request = youtube.videos().list(\n",
        "                    part='snippet,statistics,contentDetails',\n",
        "                    id=','.join(video_ids[i:i+50]))\n",
        "        response = request.execute()\n",
        "        \n",
        "        for video in response['items']:\n",
        "            video_stats = dict(\n",
        "                               Title = video['snippet']['title'],\n",
        "                               Published_date = video['snippet']['publishedAt'],\n",
        "                               Description=video['snippet']['description'],\n",
        "                               Thumbnails = video['snippet']['thumbnails']['default']['url'],\n",
        "                               Views = video['statistics']['viewCount'],\n",
        "                               Likes = video['statistics']['likeCount']\n",
        "                               #URL = video['contentDetails']\n",
        "                               #Dislikes = video['statistics']['dislikeCount'],\n",
        "                               #Comments = video['statistics']['commentCount']\n",
        "                               )\n",
        "            all_video_stats.append(video_stats)\n",
        "    \n",
        "    return all_video_stats\n",
        "video_details = get_video_details(youtube, video_ids)\n",
        "video_data = pd.DataFrame(video_details)\n",
        "\n",
        "video_data['Published_date'] = pd.to_datetime(video_data['Published_date']).dt.date\n",
        "video_data['Views'] = pd.to_numeric(video_data['Views'])\n",
        "video_data['Likes'] = pd.to_numeric(video_data['Likes'])\n",
        "#video_data['Dislikes'] = pd.to_numeric(video_data['Dislikes'])\n",
        "#video_data['Views'] = pd.to_numeric(video_data['Views'])\n",
        "#video_data\n",
        "video_data.to_csv('TED_TALKS_DATA.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zon8i3sek3vL"
      },
      "source": [
        "Installing libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42kVolupk6MV",
        "outputId": "bd59ca32-627b-49e8-a728-24792874aa54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.8/164.8 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.8/4.8 MB\u001b[0m \u001b[31m96.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.1/82.1 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for validators (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting langdetect\n",
            "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from langdetect) (1.16.0)\n",
            "Building wheels for collected packages: langdetect\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993224 sha256=3d4587465357c6b1878b5bbc0b87246e38e16d1bbbbcfd83e256c3c2b0953c0f\n",
            "  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
            "Successfully built langdetect\n",
            "Installing collected packages: langdetect\n",
            "Successfully installed langdetect-1.0.9\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q streamlit\n",
        "!pip install langdetect\n",
        "!pip install pillow"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQRTg6vllGVy"
      },
      "source": [
        "Developing GUI Interface for it\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpGPh9UglE-B",
        "outputId": "153e4f2f-97ef-4a84-b8ce-20604f56131d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Ted_Talk_Recommendation_System.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile Ted_Talk_Recommendation_System.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import plotly.express as px\n",
        "import io\n",
        "import PIL\n",
        "from PIL import Image\n",
        "\n",
        "\n",
        "import difflib\n",
        "import nltk\n",
        "import string\n",
        "import warnings\n",
        "from scipy.stats import pearsonr\n",
        "from nltk.corpus import stopwords\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        " \n",
        "nltk.download('stopwords')\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "st.set_page_config(layout = \"wide\", page_title='Radhika_1917631')\n",
        "\n",
        "st.title(\"TED Talks Recommendation System\")\n",
        "\n",
        "st.write('<p style=\"font-size:130%\">Import Dataset</p>', unsafe_allow_html=True)\n",
        "\n",
        "file_format = st.radio('Select file format:', ('csv', 'excel'))\n",
        "data = st.file_uploader(label = '')\n",
        "\n",
        "use_def = st.checkbox('Use Demo Dataset',value=True )\n",
        "if use_def:\n",
        "    data = 'TED_TALKS_DATA.csv'\n",
        "\n",
        "if data:\n",
        "    if file_format == 'csv':\n",
        "        df = pd.read_csv(data)\n",
        "    else:\n",
        "        df = pd.read_excel(data)\n",
        "    \n",
        "    st.subheader('Dataframe:')\n",
        "    n, m = df.shape\n",
        "    st.write(f'<p style=\"font-size:130%\">Dataset contains {n} rows and {m} columns.</p>', unsafe_allow_html=True)   \n",
        "    st.dataframe(df)\n",
        "    data=df\n",
        "\n",
        "#data = pd.DataFrame(data)\n",
        "#st.write(data.head())\n",
        "st.subheader(\"Language Detection\")\n",
        "from langdetect import detect\n",
        "def det(x):\n",
        "    try:\n",
        "        language = detect(x)\n",
        "    except:\n",
        "        language = 'Other'\n",
        "    return language\n",
        "df['language'] = df['Description'].apply(det)\n",
        "st.write(data)\n",
        "st.subheader(\"Filtering English language\")\n",
        "filtered_for_english = df.loc[df['language'] == 'en']\n",
        "df = df[df['language'] == 'en']\n",
        "st.write(data)\n",
        "\n",
        "data['details'] = data[\"Title\"] + ' ' + data['Description']\n",
        " \n",
        "st.subheader(\"Removing the unnecessary information\")\n",
        "#data = data[['main_speaker', 'details',\"name\",\"url\",\"title\",\"views\"]]\n",
        "data.dropna(inplace = True)\n",
        "st.write(data.head())\n",
        "\n",
        "st.subheader(\"Removing stopwords\")\n",
        "def remove_stopwords(text):\n",
        "  stop_words = stopwords.words('english')\n",
        " \n",
        "  imp_words = []\n",
        " \n",
        "  # Storing the important words\n",
        "  for word in str(text).split():\n",
        "    word = word.lower()\n",
        "     \n",
        "    if word not in stop_words:\n",
        "      imp_words.append(word)\n",
        " \n",
        "  output = \" \".join(imp_words)\n",
        " \n",
        "  return output\n",
        "data['details'] = data['details'].apply(lambda text: remove_stopwords(text))\n",
        "st.write(data.head())\n",
        "\n",
        "punctuations_list = string.punctuation\n",
        "\n",
        "\n",
        "def cleaning_punctuations(text):\n",
        "\tsignal = str.maketrans('', '', punctuations_list)\n",
        "\treturn text.translate(signal)\n",
        "\n",
        "st.subheader(\"Cleaning punctuations\")\n",
        "data['details'] = data['details'].apply(lambda x: cleaning_punctuations(x))\n",
        "st.write(data.head())\n",
        "\n",
        "details_corpus = \" \".join(data['details'])\n",
        "\n",
        "st.text(\"Training Model\")\n",
        "vectorizer = TfidfVectorizer(analyzer = 'word')\n",
        "vectorizer.fit(data['details'])\n",
        "\n",
        "def get_similarities(talk_content, data=data):\n",
        "\n",
        "\t# Getting vector for the input talk_content.\n",
        "\ttalk_array1 = vectorizer.transform(talk_content).toarray()\n",
        "\n",
        "\t# We will store similarity for each row of the dataset.\n",
        "\tsim = []\n",
        "\t#pea = []\n",
        "\tfor idx, row in data.iterrows():\n",
        "\t\tdetails = row['details']\n",
        "\n",
        "\t\t# Getting vector for current talk.\n",
        "\t\ttalk_array2 = vectorizer.transform(\n",
        "\t\t\tdata[data['details'] == details]['details']).toarray()\n",
        "\n",
        "\t\t# Calculating cosine similarities\n",
        "\t\tcos_sim = cosine_similarity(talk_array1, talk_array2)[0][0]\n",
        "\n",
        "\t\t# Calculating pearson correlation\n",
        "\t\t#pea_sim = pearsonr(talk_array1.squeeze(), talk_array2.squeeze())[0]\n",
        "\n",
        "\t\tsim.append(cos_sim)\n",
        "\t\t#pea.append(pea_sim)\n",
        "\n",
        "\treturn sim #, pea\n",
        "\n",
        "def recommend_talks(talk_content,n, data=data):\n",
        " \n",
        "    data['cos_sim'] = get_similarities(talk_content)\n",
        " \n",
        "    data.sort_values(by='cos_sim', ascending=\n",
        "                     False, inplace=True)\n",
        " \n",
        "    \n",
        "    recommended_data = data.head(n)\n",
        "    recommended_data.sort_values(by=['Views'],ascending=False)\n",
        "    r_pic = recommended_data[[\"Thumbnails\"]]\n",
        "    r_name = recommended_data[[\"Title\"]]\n",
        "    st.subheader(\"Ted Talks you might like :- \")\n",
        "    for i in range(n):\n",
        "      pic =r_pic.iloc[i][\"Thumbnails\"]\n",
        "      name = r_name.iloc[i][\"Title\"]\n",
        "      #st.write(\"check out this [link](%s)\" % url)\n",
        "      \n",
        "      st.write(\"Recommendation :- %s\" %name)\n",
        "      #image = Image.open(pic)\n",
        "      #response = requests.get(pic)\n",
        "      #img = Image.open(BytesIO(response.content))\n",
        "\n",
        "      #st.image(img, caption='Sunrise by the mountains')\n",
        "      \n",
        "      \n",
        "\n",
        "st.subheader(\"Search for your TED talk here\")\n",
        "talk_content = [st.text_input(' Enter your Ted Talk keywords : ', \"Life\")]\n",
        "n = st.number_input(' Enter number of recommendations you want ', 1)\n",
        "#talk_content = [str(input(' Enter your Ted Talk keywords : '))]\n",
        "recommend_talks(talk_content , n)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QMWnXjkqbOP",
        "outputId": "1bc0007b-1a17-47bf-badf-7d034878a5c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "added 22 packages from 22 contributors in 2.548s\n"
          ]
        }
      ],
      "source": [
        "!npm install -g localtunnel\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BFIv-IywrYiA"
      },
      "outputs": [],
      "source": [
        "import subprocess\n",
        "\n",
        "def create_tunnel(port):\n",
        "    subprocess.run(['localtunnel', str(8000)])\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "felYQmPerG8c",
        "outputId": "a15778a2-9359-40f6-ca0e-589bc4e83ef9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to False.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.31.162:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 7.136s\n",
            "your url is: https://cuddly-points-push.loca.lt\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!streamlit run Ted_Talk_Recommendation_System.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}